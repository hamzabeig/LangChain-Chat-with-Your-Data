{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "Retrieval\nRetrieval is the centerpiece of our retrieval augmented generation (RAG) flow.\n\nLet's get our vectorDB from before.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Vectorstore retrieval",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import os\nimport openai\nimport sys\nsys.path.append('../..')\n\nfrom dotenv import load_dotenv, find_dotenv\n_ = load_dotenv(find_dotenv()) # read local .env file\n\nopenai.api_key  = os.environ['OPENAI_API_KEY']",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "#!pip install lark",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "Similarity Search",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from langchain.vectorstores import Chroma\nfrom langchain.embeddings.openai import OpenAIEmbeddings\npersist_directory = 'docs/chroma/'",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "embedding = OpenAIEmbeddings()\nvectordb = Chroma(\n    persist_directory=persist_directory,\n    embedding_function=embedding\n)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "print(vectordb._collection.count())",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "texts = [\n    \"\"\"The Amanita phalloides has a large and imposing epigeous (aboveground) fruiting body (basidiocarp).\"\"\",\n    \"\"\"A mushroom with a large fruiting body is the Amanita phalloides. Some varieties are all-white.\"\"\",\n    \"\"\"A. phalloides, a.k.a Death Cap, is one of the most poisonous of all known mushrooms.\"\"\",\n]",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "smalldb = Chroma.from_texts(texts, embedding=embedding)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "question = \"Tell me about all-white mushrooms with large fruiting bodies\"",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "smalldb.similarity_search(question, k=2)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "smalldb.max_marginal_relevance_search(question,k=2, fetch_k=3)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "Addressing Diversity: Maximum marginal relevance\nLast class we introduced one problem: how to enforce diversity in the search results.\n\nMaximum marginal relevance strives to achieve both relevance to the query and diversity among the results.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "question = \"what did they say about matlab?\"\ndocs_ss = vectordb.similarity_search(question,k=3)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "docs_ss[0].page_content[:100]",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "docs_ss[1].page_content[:100]",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "Note the difference in results with MMR.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "docs_mmr = vectordb.max_marginal_relevance_search(question,k=3)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "docs_mmr[0].page_content[:100]",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "docs_mmr[1].page_content[:100]",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "Addressing Specificity: working with metadata\nIn last lecture, we showed that a question about the third lecture can include results from other lectures as well.\n\nTo address this, many vectorstores support operations on metadata.\n\nmetadata provides context for each embedded chunk.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "question = \"what did they say about regression in the third lecture?\"",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "docs = vectordb.similarity_search(\n    question,\n    k=3,\n    filter={\"source\":\"docs/cs229_lectures/MachineLearning-Lecture03.pdf\"}\n)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "for d in docs:\n    print(d.metadata)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "Addressing Specificity: working with metadata using self-query retriever\nBut we have an interesting challenge: we often want to infer the metadata from the query itself.\n\nTo address this, we can use SelfQueryRetriever, which uses an LLM to extract:\n\nThe query string to use for vector search\nA metadata filter to pass in as well\nMost vector databases support metadata filters, so this doesn't require any new databases or indexes.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from langchain.llms import OpenAI\nfrom langchain.retrievers.self_query.base import SelfQueryRetriever\nfrom langchain.chains.query_constructor.base import AttributeInfo",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "metadata_field_info = [\n    AttributeInfo(\n        name=\"source\",\n        description=\"The lecture the chunk is from, should be one of `docs/cs229_lectures/MachineLearning-Lecture01.pdf`, `docs/cs229_lectures/MachineLearning-Lecture02.pdf`, or `docs/cs229_lectures/MachineLearning-Lecture03.pdf`\",\n        type=\"string\",\n    ),\n    AttributeInfo(\n        name=\"page\",\n        description=\"The page from the lecture\",\n        type=\"integer\",\n    ),\n]",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "Note: The default model for OpenAI (\"from langchain.llms import OpenAI\") is text-davinci-003. Due to the deprication of OpenAI's model text-davinci-003 on 4 January 2024, you'll be using OpenAI's recommended replacement model gpt-3.5-turbo-instruct instead.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "document_content_description = \"Lecture notes\"\nllm = OpenAI(model='gpt-3.5-turbo-instruct', temperature=0)\nretriever = SelfQueryRetriever.from_llm(\n    llm,\n    vectordb,\n    document_content_description,\n    metadata_field_info,\n    verbose=True\n)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "question = \"what did they say about regression in the third lecture?\"",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "You will receive a warning about predict_and_parse being deprecated the first time you executing the next line. This can be safely ignored.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "docs = retriever.get_relevant_documents(question)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "for d in docs:\n    print(d.metadata)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "Additional tricks: compression\nAnother approach for improving the quality of retrieved docs is compression.\n\nInformation most relevant to a query may be buried in a document with a lot of irrelevant text.\n\nPassing that full document through your application can lead to more expensive LLM calls and poorer responses.\n\nContextual compression is meant to fix this.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from langchain.retrievers import ContextualCompressionRetriever\nfrom langchain.retrievers.document_compressors import LLMChainExtractor",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "def pretty_print_docs(docs):\n    print(f\"\\n{'-' * 100}\\n\".join([f\"Document {i+1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]))\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# Wrap our vectorstore\nllm = OpenAI(temperature=0, model=\"gpt-3.5-turbo-instruct\")\ncompressor = LLMChainExtractor.from_llm(llm)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "compression_retriever = ContextualCompressionRetriever(\n    base_compressor=compressor,\n    base_retriever=vectordb.as_retriever()\n)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "question = \"what did they say about matlab?\"\ncompressed_docs = compression_retriever.get_relevant_documents(question)\npretty_print_docs(compressed_docs)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "Combining various techniques",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "compression_retriever = ContextualCompressionRetriever(\n    base_compressor=compressor,\n    base_retriever=vectordb.as_retriever(search_type = \"mmr\")\n)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "question = \"what did they say about matlab?\"\ncompressed_docs = compression_retriever.get_relevant_documents(question)\npretty_print_docs(compressed_docs)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "Other types of retrieval\nIt's worth noting that vectordb as not the only kind of tool to retrieve documents.\n\nThe LangChain retriever abstraction includes other ways to retrieve documents, such as TF-IDF or SVM.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from langchain.retrievers import SVMRetriever\nfrom langchain.retrievers import TFIDFRetriever\nfrom langchain.document_loaders import PyPDFLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# Load PDF\nloader = PyPDFLoader(\"docs/cs229_lectures/MachineLearning-Lecture01.pdf\")\npages = loader.load()\nall_page_text=[p.page_content for p in pages]\njoined_page_text=\" \".join(all_page_text)\n\n# Split\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size = 1500,chunk_overlap = 150)\nsplits = text_splitter.split_text(joined_page_text)\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# Retrieve\nsvm_retriever = SVMRetriever.from_texts(splits,embedding)\ntfidf_retriever = TFIDFRetriever.from_texts(splits)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "question = \"What are major topics for this class?\"\ndocs_svm=svm_retriever.get_relevant_documents(question)\ndocs_svm[0]",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "question = \"what did they say about matlab?\"\ndocs_tfidf=tfidf_retriever.get_relevant_documents(question)\ndocs_tfidf[0]",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}